# -*- coding: utf-8 -*-
"""Women Cloth Reviews.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tk9rAVWJ8eWF8lttsX-G8pUktElyKv_y

# **Women Cloth Reviews**

# **Objective**
Multinomial Naive Bayes algorithm is a probabilistic learning method that is mostly used in Natural Language Processing. It is also suitable for text classification with discrete feature. In this project it will be used to build a women cloth review prediction model.

# **Data Source**
https://raw.githubusercontent.com/YBIFoundation/ProjectHub-MachineLearning/main/Women%20Clothing%20E-Commerce%20Review.csv
"""

# @title Import Library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import itertools

# @title Import Data
df=pd. read_csv('https://raw.githubusercontent.com/YBI-Foundation/Teaching-Data/main/Women%20Clothing%20E-Commerce%20Review.csv')

df.head()

df.info()

df.shape

#  @title Describe data
data_path = 'https://raw.githubusercontent.com/YBI-Foundation/Teaching-Data/main/Women%20Clothing%20E-Commerce%20Review.csv'
df = pd.read_csv(data_path)
df.head()

df['Recommended']

print(df.describe())
print("\n \n Shape of the data :", df.shape)
print("\n \n The info of the data :", df.info)

df.describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu').format('{:.2f}')

df.describe(include=object).T

# @title Data Visualization
colnames = pd.read_csv('https://raw.githubusercontent.com/YBI-Foundation/Teaching-Data/main/Women%20Clothing%20E-Commerce%20Review.csv') # Fix the URL by changing 'ttps' to 'https'
colnames.columns = ('Clothing ID', 'Age', 'Title', 'Review', 'Rating', 'Recommend', 'Positive Feedback', 'Division', 'Department', ' Category ')

str(colnames)

object_columns = df.select_dtypes(include='object').columns
df_numeric = df.drop(columns=object_columns)
matrix = np.triu(df_numeric.corr())
plt.figure(figsize=(14, 10))
sns.heatmap(df_numeric.corr(), annot=True, cmap=sns.cubehelix_palette(8), mask=matrix)
plt.xticks(rotation=45);

# @title Recommended
df.columns

df["Recommended"].value_counts()

df["Recommended"].value_counts()

first_look("Recommended")

df["Recommended"].describe().T

sns.countplot(x = df.Recommended, data = df)
plt.title('Customer Recommendation Distribution', fontsize=15)
plt.xlabel("Recommendation Label", fontsize=15)
plt.ylabel(" The Number of Recommendations", fontsize=15)

for index,value in enumerate(df.Recommended.value_counts().sort_values()):
     plt.text(index, value, f"{value}", ha="center", va="bottom", fontsize = 13);



plt.figure(figsize=(8, 8))

explode = [0, 0.1]
plt.pie(df['Recommended'].value_counts(), explode=explode, autopct='%1.1f%%', shadow=True, startangle=140)
plt.legend(labels=['1', '0'])
plt.title('Customer Recommendation Distribution', fontsize=20)
plt.axis('off');

# @title Age
df["Age"].value_counts()

first_look("Age")

df["Age"].describe().T

plt.figure(figsize = (20, 8))
plt.title('Customer Age Distribution', fontsize=30)
plt.xlabel("Age", fontsize=24)
plt.ylabel("The Number of Customer Age", fontsize=18)

sns.histplot(df, x='Age', kde = True, bins = 50);

# @title Positive Feedback
df["Positive Feedback"].value_counts()

first_look("Positive Feedback")

df["Positive Feedback"].describe().T

plt.figure(figsize = (20, 8))
plt.title('Customer Positive Feedback Distribution', fontsize=20)
plt.xlabel("Customer Positive Feedback", fontsize=24)
plt.ylabel("The Number of Customer Positive Feedback", fontsize=18)
sns.histplot(df, x='Positive Feedback', kde = True, bins = 50);

# @title Division
df["Division"].value_counts()

first_look("Division")

df["Division"].describe().T

g = sns.catplot( x='Division',
             kind="count",
             data=df,
             height=5,
             aspect=2)
plt.title('Division Distribution', fontsize=24)
plt.xlabel("Division Name", fontsize=24)
plt.ylabel("The Number of Divisions", fontsize=20)
ax = g.facet_axis(0, 0)
for p in ax.patches:
    ax.text(p.get_x() + 0.28,
            p.get_height() * 1.025,
            '{0:.0f}'.format(p.get_height()),
            color='black', rotation='horizontal', size='large')

# @title Rating
df["Rating"].value_counts()

first_look("Rating")

df["Rating"].describe().T

sns.countplot(x = df.Rating, data = df)
plt.title('Customer Rating Distribution', fontsize=30)
plt.xlabel("Rating Label", fontsize=24)
plt.ylabel("The Number of Rating", fontsize=24)
for index,value in enumerate(df.Rating.value_counts().sort_values()):
     plt.text(index, value, f"{value}", ha="center", va="bottom", fontsize = 13);

# @title Department
df["Department"].value_counts()

first_look("Department")

g = sns.catplot(data = df, x ="Department", kind='count', height=5, aspect=2)
plt.title('Department Distribution', fontsize=26)
plt.xlabel("Department Name", fontsize=20)
plt.ylabel("The Number of Departments", fontsize=20)
ax = g.facet_axis(0, 0)
for p in ax.patches:
    ax.text(p.get_x() + 0.28,
            p.get_height() * 1.025,
            '{0:.0f}'.format(p.get_height()),
            color='black', rotation='horizontal', size='large')
    plt.show()

# @title Category
df["Category"].value_counts()

first_look("Category")

df["Category"].describe().T

plt.title('Product Class Distribution', fontsize=25)
df["Category"].value_counts().plot(kind="pie", autopct='%1.1f%%', figsize=(16, 11));

# @title Data preprocessing
df.isna().sum()

df.nunique()

import re
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
stop_words = set(stopwords.words('english'))

def preprocess(raw_text):
    letters_only_text = re.sub("[^a-zA-Z]", " ", raw_text)

    words = letters_only_text.lower().split()

    cleaned_words = []
    lemmatizer = WordNetLemmatizer()

    for word in words:
        if word not in stop_words:
            cleaned_words.append(word)

    lemmas = []
    for word in cleaned_words:
        word = lemmatizer.lemmatize(word)
        lemmas.append(word)

    return " ".join(lemmas)


df['Review'] = df['Review'].astype(str)
df['clean_review_text'] = df['Review'].apply(preprocess)
df[['Review','clean_review_text']].head(3)

# @title Define Target Variable (y) and Feature Variables (X)
df.columns

x = df['Review']

y = df['Rating']

df['Rating'].value_counts()

#  @title Train Test Split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, train_size = 0.7, stratify = y, random_state = 2529)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

# @title Modeling
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()
X_train_transformed = vectorizer.fit_transform(X_train)
X_test_transformed = vectorizer.transform(X_test)

model = MultinomialNB()
model.fit(X_train_transformed, y_train)

# @title Model prediction
X_test_transformed = vectorizer.transform(X_test)
y_pred = model.predict(X_test_transformed)
y_pred.shape

y_pred

model.predict_proba(X_test_transformed)

# @title Model evaluation
from sklearn.metrics import confusion_matrix, classification_report
print(confusion_matrix(y_test, y_pred))

print(classification_report(y_test, y_pred))

"""# **Explanation**
This project focuses on building predictive models. First, both the required libraries and the test data set are introduced. The data set was observed and pre-processed to prepare it for its use, then part of it was kept for testing and the rest was used to train the model and use the model to derive a data set of predictions. Finally, the prediction accuracy was checked using the test data set, some adjustments were made, and the model was retrained to achieve better accuracy. This project focuses on building predictive models. First, both the required libraries and the test data set are introduced. The data set was observed and pre-processed to prepare it for its use, then part of it was kept for testing and the rest was used to train the model and use the model to derive a data set of predictions. Finally, the prediction accuracy was checked using the test data set, some adjustments were made, and the model was retrained to achieve better accuracy.
"""